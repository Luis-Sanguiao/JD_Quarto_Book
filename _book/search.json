[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JDemetra+ online documentation",
    "section": "",
    "text": "Welcome to the JDemetra+ online documentation.\nJDemetra+ is a software for seasonal adjustment and other time series functions, developed in Eurostat’s “Centre of Excellence on Statistical Methods and Tools”.\nTo learn more about this project https://ec.europa.eu/eurostat/cros/content/centre-excellence-statistical-methods-and-tools."
  },
  {
    "objectID": "G-what-is-jd.html",
    "href": "G-what-is-jd.html",
    "title": "1  JDemetra+ Software",
    "section": "",
    "text": "ressources for descrption - R tools WP - desp in esp"
  },
  {
    "objectID": "G-what-is-jd.html#a-library-of-algorithms-for-time-series-related-functions-needs",
    "href": "G-what-is-jd.html#a-library-of-algorithms-for-time-series-related-functions-needs",
    "title": "1  JDemetra+ Software",
    "section": "1.1 A library of algorithms for time series related functions ? needs ?",
    "text": "1.1 A library of algorithms for time series related functions ? needs ?\nYou can learn more about the history of the project here (link to below)"
  },
  {
    "objectID": "G-what-is-jd.html#structure-of-this-book",
    "href": "G-what-is-jd.html#structure-of-this-book",
    "title": "1  JDemetra+ Software",
    "section": "1.2 Structure of this book",
    "text": "1.2 Structure of this book\nThis book is divided in four parts, each being an entry point for the user.\n\n1.2.1 Algorithms\nThis part provides a step by step description of all the algorithms featured in JD+, grouped by purpose - seasonal adjstement - benchmarking - temporal disaggregation - … links\n\n\n1.2.2 Tools\nJdemetra+ offers 3 kind of tools\n\n\n1.2.3 Underlying Statistical Methods\nThis part gives details about the underlying statistical methods to foster a more in-depth understanding of the algorithms.Those methods are described in the light and spirit of their use as buliding blocks of the algorithms presented above, not aiming at all at their comprehensive coverage."
  },
  {
    "objectID": "G-what-is-jd.html#how-to-use-this-book",
    "href": "G-what-is-jd.html#how-to-use-this-book",
    "title": "1  JDemetra+ Software",
    "section": "1.3 How to use this book",
    "text": "1.3 How to use this book\naudience: book targets the beginner as well as seasoned (pun moethodlogist. The beginner is advised to use the quick start chapter as an etrey point, it’s presneted like a decsion tree which will point directly to the the part it’s useful to review.\nthe seasoned methodologist will benefit from the detailed chapter ad part strucutre to quickly find the needed information."
  },
  {
    "objectID": "G-quick-start.html",
    "href": "G-quick-start.html",
    "title": "2  Quick start with…",
    "section": "",
    "text": "objective: describe key steps + provide useful liks to relevant code"
  },
  {
    "objectID": "G-quick-start.html#seasonal-adjustment",
    "href": "G-quick-start.html#seasonal-adjustment",
    "title": "2  Quick start with…",
    "section": "2.1 Seasonal Adjustment",
    "text": "2.1 Seasonal Adjustment"
  },
  {
    "objectID": "G-quick-start.html#seasonal-adjustment-of-high-frequency-data",
    "href": "G-quick-start.html#seasonal-adjustment-of-high-frequency-data",
    "title": "2  Quick start with…",
    "section": "2.2 Seasonal Adjustment of High-Frequency Data",
    "text": "2.2 Seasonal Adjustment of High-Frequency Data"
  },
  {
    "objectID": "G-quick-start.html#use-of-jd-algorithms-in-r",
    "href": "G-quick-start.html#use-of-jd-algorithms-in-r",
    "title": "2  Quick start with…",
    "section": "2.3 Use of JD+ algorithms in R",
    "text": "2.3 Use of JD+ algorithms in R"
  },
  {
    "objectID": "G-quick-start.html#use-of-jd-graphical-interface",
    "href": "G-quick-start.html#use-of-jd-graphical-interface",
    "title": "2  Quick start with…",
    "section": "2.4 Use of JD+ graphical interface",
    "text": "2.4 Use of JD+ graphical interface"
  },
  {
    "objectID": "G-main-features.html",
    "href": "G-main-features.html",
    "title": "3  Main functions overview",
    "section": "",
    "text": "link to key references * 2 handbooks * sets of guidelines\nObjective: present JDemetra+ capabilities by category"
  },
  {
    "objectID": "G-main-features.html#seasonal-adjustment-algorithms",
    "href": "G-main-features.html#seasonal-adjustment-algorithms",
    "title": "3  Main functions overview",
    "section": "3.1 Seasonal adjustment algorithms",
    "text": "3.1 Seasonal adjustment algorithms\nbelow: pieces of old pages to edit and update\n\n3.1.1 Data frequencies\nThe seasonal adjustment methods available in JDemetra+ aim to decompose a time series into components and remove seasonal fluctuations from the observed time series. The X-11 method considers monthly and quarterly series while SEATS is able to decompose series with 2, 3, 4, 6 and 12 observations per year.\n\n\n3.1.2 X-13\nX-13ARIMA is a seasonal adjustment program developed and supported by the U.S. Census Bureau. It is based on the U.S. Census Bureau's earlier X-11 program, the X-11-ARIMA program developed at Statistics Canada, the X-12-ARIMA program developed by the U.S. Census Bureau, and the SEATS program developed at the Banco de España. The program is now used by the U.S. Census Bureau for a seasonal adjustment of time series.\n\n\n3.1.3 Tramo-Seats\n\n\n3.1.4 STL\n\n\n3.1.5 Basic Structural Models"
  },
  {
    "objectID": "G-main-features.html#trend-cycle-estimation",
    "href": "G-main-features.html#trend-cycle-estimation",
    "title": "3  Main functions overview",
    "section": "3.2 Trend-cycle estimation",
    "text": "3.2 Trend-cycle estimation"
  },
  {
    "objectID": "G-main-features.html#nowacsting",
    "href": "G-main-features.html#nowacsting",
    "title": "3  Main functions overview",
    "section": "3.3 Nowacsting",
    "text": "3.3 Nowacsting"
  },
  {
    "objectID": "G-main-features.html#temporal-disaggregation",
    "href": "G-main-features.html#temporal-disaggregation",
    "title": "3  Main functions overview",
    "section": "3.4 Temporal Disaggregation",
    "text": "3.4 Temporal Disaggregation"
  },
  {
    "objectID": "A-sa.html",
    "href": "A-sa.html",
    "title": "4  Seasonal Adjustment",
    "section": "",
    "text": "The primary aim of the seasonal adjustment process is to remove seasonal fluctuations from the time series. To achieve this goal, seasonal adjustment methods decompose the original time series into components that capture specific movements. These components are: trend-cycle, seasonality and irregularity. The trend-cycle component includes long-term and medium-term movements in the data. For seasonal adjustment purposes there is no need to divide this component into two parts. JDemetra+ refers to the trend-cycle as trend and consequently this convention is used here."
  },
  {
    "objectID": "A-outlier-detection.html",
    "href": "A-outlier-detection.html",
    "title": "5  Outlier detection",
    "section": "",
    "text": "in or outside a seasonal adjustment process"
  },
  {
    "objectID": "A-outlier-detection.html#motivation",
    "href": "A-outlier-detection.html#motivation",
    "title": "5  Outlier detection",
    "section": "5.1 Motivation",
    "text": "5.1 Motivation"
  },
  {
    "objectID": "A-outlier-detection.html#with-reg-arima-models",
    "href": "A-outlier-detection.html#with-reg-arima-models",
    "title": "5  Outlier detection",
    "section": "5.2 With Reg Arima models",
    "text": "5.2 With Reg Arima models\n\n5.2.1 Part of preadjustment\n\n\n5.2.2 Specific TERROR tool"
  },
  {
    "objectID": "A-outlier-detection.html#with-structural-models",
    "href": "A-outlier-detection.html#with-structural-models",
    "title": "5  Outlier detection",
    "section": "5.3 With structural models",
    "text": "5.3 With structural models"
  },
  {
    "objectID": "A-calendar-and-input.html",
    "href": "A-calendar-and-input.html",
    "title": "6  Calendar correction and user-defined corrections",
    "section": "",
    "text": "generating Calendar regressors and other input variables"
  },
  {
    "objectID": "A-calendar-and-input.html#motivation",
    "href": "A-calendar-and-input.html#motivation",
    "title": "6  Calendar correction and user-defined corrections",
    "section": "6.1 Motivation",
    "text": "6.1 Motivation"
  },
  {
    "objectID": "A-calendar-and-input.html#underlying-theory",
    "href": "A-calendar-and-input.html#underlying-theory",
    "title": "6  Calendar correction and user-defined corrections",
    "section": "6.2 Underlying theory",
    "text": "6.2 Underlying theory\n\n6.2.1 Overview of Calendar effects in JDemetra+\nThe following description of the calendar effects in JDemetra+ is strictly based on PALATE, J. (2014).\nA natural way for modelling calendar effects consists of distributing the days of each period into different groups. The regression variable corresponding to a type of day (a group) is simply defined by the number of days it contains for each period. Usual classifications are:\n\nTrading days (7 groups): each day of the week defines a group (Mondays,...,Sundays);\nWorking days (2 groups): week days and weekends.\n\nThe definition of a group could involve partial days. For instance, we could consider that one half of Saturdays belong to week days and the second half to weekends.\nUsually, specific holidays are handled as Sundays and they are included in the group corresponding to \"non-working days\". This approach assumes that the economic activity on national holidays is the same (or very close to) the level of activity that is typical for Sundays. Alternatively, specific holidays can be considered separately, e.g. by the specification that divided days into three groups:\n\nWorking days (Mondays to Fridays, except for specific holidays),\nNon-working days (Saturdays and Sundays, except for specific holidays),\nSpecific holidays."
  },
  {
    "objectID": "A-sa-hf.html#tools",
    "href": "A-sa-hf.html#tools",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.2 Tools",
    "text": "7.2 Tools\ncode here and/or link to R packages chapter"
  },
  {
    "objectID": "A-sa-hf.html#unobserved-components",
    "href": "A-sa-hf.html#unobserved-components",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.3 Unobserved Components",
    "text": "7.3 Unobserved Components"
  },
  {
    "objectID": "A-sa-hf.html#seasonality-tests",
    "href": "A-sa-hf.html#seasonality-tests",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.4 Seasonality tests",
    "text": "7.4 Seasonality tests\n\n7.4.1 Spectral analysis"
  },
  {
    "objectID": "A-sa-hf.html#pre-adjustment",
    "href": "A-sa-hf.html#pre-adjustment",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.5 Pre-adjustment",
    "text": "7.5 Pre-adjustment\n\n7.5.1 Calendar correction\n\n\n7.5.2 Outliers and intervention variables\n\n\n7.5.3 Linearization"
  },
  {
    "objectID": "A-sa-hf.html#decomposition",
    "href": "A-sa-hf.html#decomposition",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.6 Decomposition",
    "text": "7.6 Decomposition\n\n7.6.1 Extended X-11\n\n\n7.6.2 STL decomposition\n\n\n7.6.3 Arima Model Based (AMB) Decomposition\n\n\n7.6.4 State Space framework"
  },
  {
    "objectID": "A-sa-hf.html#quality-assessment",
    "href": "A-sa-hf.html#quality-assessment",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.7 Quality assessment",
    "text": "7.7 Quality assessment\n\n7.7.1 Residual seasonality\n\n\n7.7.2 Residual Calendar effects\n\n\n7.7.3 Arima Model"
  },
  {
    "objectID": "A-sa-hf.html#forecasting",
    "href": "A-sa-hf.html#forecasting",
    "title": "7  Seasonal adjustment of high frequency data",
    "section": "7.8 Forecasting",
    "text": "7.8 Forecasting"
  },
  {
    "objectID": "A-benchmarking.html",
    "href": "A-benchmarking.html",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "",
    "text": "Often one has two (or multiple) datasets of different frequency for the same target variable. Sometimes, however, these datasets are not coherent in the sense that they don’t match up. Benchmarking[^1] is a method todeal with this situation. An aggregate of a higher-frequency measurement variables is not necessarily equal to the corresponding lower-frequency less-aggregated measurement. Moreover, the sources of data may have different reliability levels. Usually, less frequent data are considered more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurements, hence often the less frequent, will serve as benchmark.\nIn seasonal adjustment methods benchmarking is the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. The U.S. Census Bureau also points out that “forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero”[^2]. Nevertheless, some users may need that the annual totals of the seasonally adjusted series match the annual totals of the original, non-seasonally adjusted series[^3].\nAccording to the [ESS Guidelines on Seasonal Adjustment (2015)] (https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3), the only benefit of this approach is that there is consistency over the year between adjusted and the non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) and where users’ needs for time consistency are stronger."
  },
  {
    "objectID": "A-benchmarking.html#underlying-theory",
    "href": "A-benchmarking.html#underlying-theory",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.2 Underlying Theory",
    "text": "8.2 Underlying Theory\nBenchmarking1 is a procedure widely used when for the same target variable the two or more sources of data with different frequency are available. Generally, the two sources of data rarely agree, as an aggregate of higher-frequency measurements is not necessarily equal to the less-aggregated measurement. Moreover, the sources of data may have different reliability. Usually it is thought that less frequent data are more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurement is considered as a benchmark.\nBenchmarking also occurs in the context of seasonal adjustment. Seasonal adjustment causes discrepancies between the annual totals of the seasonally unadjusted (raw) and the corresponding annual totals of the seasonally adjusted series. Therefore, seasonally adjusted series are benchmarked to the annual totals of the raw time series2. Therefore, in such a case benchmarking means the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the ‘ESS Guidelines on Seasonal Adjustment’ (2015) do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. Also the U.S. Census Bureau points out that: Forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero.3 Nevertheless, some users may prefer the annual totals for the seasonally adjusted series to match the annual totals for the original, non-seasonally adjusted series4. According to the ‘ESS Guidelines on Seasonal Adjustment’ (2015), the only benefit of this approach is that there is consistency over the year between adjusted and non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) where user needs for time consistency are stronger.\nThe benchmarking procedure in JDemetra+ is available for a single seasonally adjusted series and for an indirect seasonal adjustment of an aggregated series. In the first case, univariate benchmarking ensures consistency between the raw and seasonally adjusted series. In the second case, the multivariate benchmarking aims for consistency between the seasonally adjusted aggregate and its seasonally adjusted components.\nGiven a set of initial time series \\[\\left\\{ z_{i,t} \\right\\}_{i \\in I}\\], the aim of the benchmarking procedure is to find the corresponding\n\\[\\left\\{ x_{i,t} \\right\\}_{i \\in I}\\] that respect temporal aggregation constraints, represented by \\[X_{i,T} = \\sum_{t \\in T}^{}x_{i,t}\\] and contemporaneous constraints given by \\[q_{k,t} = \\sum_{j \\in J_{k}}^{}{w_{\\text{kj}}x_{j,t}}\\] or, in matrix\nform: \\[q_{k,t} = w_{k}x_{t}\\].\nThe underlying benchmarking method implemented in JDemetra+ is an extension of Cholette's5 method, which generalises, amongst others, the additive and the multiplicative Denton procedure as well as simple proportional benchmarking.\nThe JDemetra+ solution uses the following routines that are described in DURBIN, J., and KOOPMAN, S.J. (2001):\n\nThe multivariate model is handled through its univariate transformation,\nThe smoothed states are computed by means of the disturbance smoother.\n\nThe performance of the resulting algorithm is highly dependent on the number of variables involved in the model (\\(\\propto \\ n^{3}\\)). The other components of the problem (number of constraints, frequency of the series, and length of the series) are much less important (\\(\\propto \\ n\\)).\nFrom a theoretical point of view, it should be noted that this approach may handle any set of linear restrictions (equalities), endogenous (between variables) or exogenous (related to external values), provided that they don’t contain incompatible equations. The restrictions can also be relaxed for any period by considering their \"observation\" as missing. However, in practice, it appears that several kinds of contemporaneous constraints yield unstable results. This is more especially true for constraints that contain differences (which is the case for non-binding constraints). The use of a special square root initialiser improves in a significant way the stability of the algorithm."
  },
  {
    "objectID": "A-benchmarking.html#tools",
    "href": "A-benchmarking.html#tools",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.3 Tools",
    "text": "8.3 Tools"
  },
  {
    "objectID": "A-benchmarking.html#references",
    "href": "A-benchmarking.html#references",
    "title": "8  Benchmarking and temporal disagreggation",
    "section": "8.4 References",
    "text": "8.4 References"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#underlying-theory",
    "href": "A-trend-cycle-estimation.html#underlying-theory",
    "title": "9  Trend-cycle estimation",
    "section": "9.2 Underlying Theory",
    "text": "9.2 Underlying Theory"
  },
  {
    "objectID": "A-trend-cycle-estimation.html#tools",
    "href": "A-trend-cycle-estimation.html#tools",
    "title": "9  Trend-cycle estimation",
    "section": "9.3 Tools",
    "text": "9.3 Tools\n\n9.3.1 rjdfilters package"
  },
  {
    "objectID": "A-nowcasting.html#underlying-theory",
    "href": "A-nowcasting.html#underlying-theory",
    "title": "10  Nowcasting",
    "section": "10.2 Underlying Theory",
    "text": "10.2 Underlying Theory"
  },
  {
    "objectID": "A-nowcasting.html#tools",
    "href": "A-nowcasting.html#tools",
    "title": "10  Nowcasting",
    "section": "10.3 Tools",
    "text": "10.3 Tools"
  },
  {
    "objectID": "T-graphical-user-interface.html",
    "href": "T-graphical-user-interface.html",
    "title": "11  Graphical User Interface",
    "section": "",
    "text": "why use the graphical user interface ? what is not directly available in R yet?\nobjective: describe the general features (independent of algorithms) - general layout - import data - documents - workspaces - specifications - output\nold content can be recycled but - very heavy (trim from md or txt files) - check version 3 - see if we stick with pasted screen shots"
  },
  {
    "objectID": "T-r-packages.html",
    "href": "T-r-packages.html",
    "title": "12  R packages",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "T-r-packages.html#organisation-overview",
    "href": "T-r-packages.html#organisation-overview",
    "title": "12  R packages",
    "section": "12.2 Organisation overview",
    "text": "12.2 Organisation overview\na suite (order)\ngeneral output organisation"
  },
  {
    "objectID": "T-r-packages.html#installation-procedure",
    "href": "T-r-packages.html#installation-procedure",
    "title": "12  R packages",
    "section": "12.3 Installation procedure",
    "text": "12.3 Installation procedure"
  },
  {
    "objectID": "T-r-packages.html#interaction-with-gui",
    "href": "T-r-packages.html#interaction-with-gui",
    "title": "12  R packages",
    "section": "12.4 Interaction with GUI",
    "text": "12.4 Interaction with GUI"
  },
  {
    "objectID": "T-r-packages.html#full-list",
    "href": "T-r-packages.html#full-list",
    "title": "12  R packages",
    "section": "12.5 Full list",
    "text": "12.5 Full list\n\n12.5.1 rjd3modelling\nmain functions: table"
  },
  {
    "objectID": "T-plug-ins.html",
    "href": "T-plug-ins.html",
    "title": "13  Plug-ins for JDemetra+",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "T-production.html",
    "href": "T-production.html",
    "title": "14  Production",
    "section": "",
    "text": "obj here: general explanations + examples ? here : explain voc discrepancies vs guidelines bbk controlled current link to plug in illustration links on covid\nJDemetra+ offers several options for refreshing the output, which are in line with the ESS Guidelines on Seasonal Adjustment (2015) (link) requirements.\nreprduce table cf. my pdf (xls) doc remark: rjwsa cruncher vignette is not up to date"
  },
  {
    "objectID": "T-tool-selection.html",
    "href": "T-tool-selection.html",
    "title": "15  Tool selection issues",
    "section": "",
    "text": "objective : select wisely between GUI(+ cruncher and plug ins) and R packages"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-analysis-concepts-and-overview",
    "href": "M-spectral-analysis.html#spectral-analysis-concepts-and-overview",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.2 Spectral analysis concepts and overview",
    "text": "16.2 Spectral analysis concepts and overview\nA time series \\(x_{t}\\) with stationary covariance, mean \\(μ\\) and \\(k^{th}\\) autocovariance \\(E(x_{t}-\\mu)(x_{t- k}\\mu))=\\gamma(k)\\) can be described as a weighted sum of periodic trigonometric functions: \\(sin(\\omega t)\\) and \\(cos(\\omega t)\\), where \\(\\omega=\\frac{2*pi}{T}\\) denotes frequency. Spectral analysis investigates this frequency domain representation of \\(x_{t}\\) to determine how important cycles of different frequencies are in accounting for the behavior of \\(x_{t}\\).\nAssuming that the autocovariances \\(\\gamma(k)\\) are absolutely summable(\\(\\sum_{k =-\\infty}^{\\infty}|\\gamma(k)|<\\infty\\)), the autocovariance generating function, which summarizes these autocovariances through a scalar valued function, is given by equation [1]1.\n\\(acgf(z)=\\sum_{k=-\\infty}^{\\infty}{z^{k}\\gamma(k)}\\),\nwhere \\(z\\) denotes complex scalar.\nOnce the equation [1]is divided by \\(\\pi\\) and evaluated at some \\(z{= e}^{- i\\omega} = cos\\omega - isin\\omega\\), where \\(i = \\sqrt{- 1}\\) and \\(\\omega\\) is a real scalar,\\(\\  - \\infty < \\ \\omega < \\infty\\), the result of this transformation is called a population spectrum $f()$for \\(\\ x_{t}\\), given in equation [2]2.\n\\[f(\\omega) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{e^{- ik\\omega}\\gamma(k)}\\]\nTherefore, the analysis of the population spectrum in the frequency domain is equivalent to the examination of the autocovariance function in the time domain analysis; however it provides an alternative way of inspecting the process. Because \\(f(\\omega)\\text{dω}\\) is interpreted as a contribution to the variance of components with frequencies in the range \\((\\omega,\\ \\omega + d\\omega)\\), a peak in the spectrum indicates an important contribution to the variance at frequencies near the value that corresponds to this peak.\nAs $e^{- i} = cos- isin,$the spectrum can be also expressed as in equation [3].\n\\[f(\\omega) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{(cos\\omega k - isin\\omega k)\\gamma(k)}\\]\nSince \\(\\gamma(k) = \\gamma( - k)\\) (i.e. $(k)$is an even function of \\(k\\)) and \\(\\sin{( - x)}\\  = \\operatorname{-sin}x\\), [3] can be presented as equation\n\\[f(\\omega) = \\frac{1}{\\pi}\\lbrack \\ \\gamma(0) + 2\\sum_{k = 1}^{\\infty}{\\ \\gamma(k)}cos\\text{ωk} \\rbrack\\],\nThis implies that if autocovariances are absolutely summable the population spectrum exists and is a continuous, real-valued function of \\(\\omega\\). Due to the properties of trigonometric functions $(( - k) = () . $and $.  (+ 2j)k = cos(k))$the spectrum is a periodic, even function of \\(\\omega\\), symmetric around \\(\\omega = 0\\). Therefore, the analysis of the spectrum can be reduced to the interval \\(( - \\pi,\\pi).\\) The spectrum is non-negative for all \\(\\omega \\in ( - \\pi,\\pi)\\).\nThe shortest cycle that can be distinguished in a time series lasts two periods. The frequency which corresponds to this cycle is \\(\\omega = \\pi\\) and is called the Nyquist frequency. The frequency of the longest cycles that can be observed in the time series with \\(n\\) observations is \\(\\omega = \\frac{2\\pi}{n}\\) and is called the fundamental (Fourier) frequency.\nNote that if \\(x_{t}\\) is a white noise process with zero mean and variance \\(\\sigma^{2}\\), then for all \\(|k|> 0\\) \\(\\gamma(k)=0\\) and the spectrum of \\(x_{t}\\) is constant (\\(f(\\omega)= \\frac{\\sigma^{2}}{\\pi}\\)) since each frequency in the spectrum contributes equally to the variance of the process3.\nThe aim of spectral analysis is to determine how important cycles of different frequencies are in accounting for the behaviour of a time series4. Since spectral analysis can be used to detect the presence of periodic components, it is a natural diagnostic tool for detecting trading day effects as well as seasonal effects5. Among the tools used for spectral analysis are the autoregressive spectrum and the periodogram.\nThe explanations given in the subsections of this node derive mainly from DE ANTONIO, D., and PALATE, J. (2015) and BROCKWELL, P.J., and DAVIS, R.A. (2006).\ncomment1: end old intro: ok\n\n16.2.1 Theoretical spectral density of an ARIMA model"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-density-estimation",
    "href": "M-spectral-analysis.html#spectral-density-estimation",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.3 Spectral density estimation",
    "text": "16.3 Spectral density estimation\n\n16.3.1 Method 1: The periodogram\nFor any given frequency \\(\\omega\\) the sample periodogram is the sample analog of the sample spectrum. In general, the periodogram is used to identify the periodic components of unknown frequency in the time series. X-13ARIMA-SEATS and TRAMO-SEATS use this tool for detecting seasonality in raw time series and seasonally adjusted series. Apart from this it is applied for checking randomness of the residuals from the ARIMA model.\nTo define the periodogram, first consider the vector of complex numbers6:\n$$ =\n\\[\\begin{bmatrix}      \n\n\n  x_{1} \\\\                             \n  x_{2} \\\\                             \n  . \\\\                                 \n  . \\\\                                 \n  . \\\\                                 \n  x_{n} \\\\                             \n  \\end{bmatrix}\\]\n^{n}$$\nwhere \\(\\mathbb{C}^{n}\\) is the set of all column vectors with complex-valued components.\nThe Fourier frequencies associated with the sample size \\(n\\) are defined as a set of values \\(ω_{j} = \\frac{2\\pi j}{n}\\), \\(j = - \\lbrack \\frac{n-1}{2}\\rbrack,\\ldots,\\lbrack\\frac{n}{2}\\rbrack\\), \\(-\\pi< \\omega_{j} \\leq \\pi\\), \\(j\\in F_{n}\\), where \\({\\lbrack n\\rbrack}\\) denotes the largest integer less than or equal to \\(n\\). The Fourier frequencies, which are called harmonics, are given by integer multiples of the fundamental frequency \\(\\ \\frac{2\\pi}{n}\\).\nNow the \\(n\\) vectors \\[e_{j} = n^{- \\frac{1}{2}}(e^{-i\\omega_{j}},e^{-i{2\\omega}_{j}},\n\\ldots,e^{- inω_{j}})^{'}\\] can be defined. Vectors \\[e_{1},\\ldots, e_{n}\\] are orthonormal in the sense that:\n\\[\n{\\mathbf{e}_{j}^{*}\\mathbf{e}}_{k} = n^{- 1}\\sum_{r = 1}^{n}e^{ir(\\omega_{j} - \\omega_{k})} = { \\begin{matrix}  \n  1,\\ if\\ j = k \\\\                                                                                                         \n  0,\\ if\\ j \\neq k \\\\                                                                                                      \n  \\end{matrix} .\\\n\\]\nwhere \\[\\mathbf{e}_{j}^{*}\\] denotes the row vector, which \\[k^{th}\\] component is the complex conjugate of the \\[k^{th}\\] component of \\[\\mathbf{e}_{j}\\].7 These vectors are a basis of \\[F_{n}\\], so that any \\[\\mathbf{x}\\in\\mathbb{C}^{n}\\] can be expressed as a sum of \\[n\\] components:\n\\[\n\\mathbf{x} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{a_{j}\\mathbf{e}_{j}}\n\\]\nwhere the coefficients \\[a_{j} = \\mathbf{e}_{j}^{*}\\mathbf{x}=n^{-\\frac{1}{2}}\\sum_{t = 1}^{n}x_{t}e^{-it\\omega_{j}}\\] are derived from [3] by multiplying the equation on the left by \\[\\mathbf{e}_{j}^{*}\\] and using [1].\nThe sequence of \\(\\{a_{j},j\\in F_{n}\\}\\) is referred as a discrete Fourier transform of \\(\\mathbf{x}\\mathbb{\\in C}^{n}\\) and the periodogram \\(I(\\omega_{j})\\) of \\(\\mathbf{x}\\) at Fourier frequency \\(\\omega_{j} = \\frac{2\\pi j}{n}\\) is defined as the square of the Fourier transform \\[\\{a_{j}\\}\\] of \\(\\mathbf{x}\\):\n\\[\n{I(\\omega_{j})\\mathbf{=}{| a_{j} |^{2}}_{\\ } = n^{- \\ 1}| \\sum_{t = 1}^{n}x_{t}e^{- it\\omega_{j}} |^{2}}_{\\mathbf{\\ }}\n\\]\nFrom [2] and [3] it can be shown that in fact the periodogram decomposes the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) into a sums of components associated with the Fourier frequencies \\[ω_{j}\\]:\n\\[\n  \\sum_{t=1}^{n}{|x_{t}|}^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}|a_{j}|^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{I(\\omega_{j})}\n\\]\nIf \\(\\ \\mathbf{x\\  \\in}\\ {R}^{n}\\), \\(\\omega_{j}\\) and \\[{-\\omega}_{j}\\] are both in \\[\\lbrack- \\pi, -\\pi \\rbrack\\] and \\[a_{j}\\] is presented in its polar form (i.e.\\[a_{j} = r_{j}\\exp( i\\theta_{j})\\]), where \\(r_{j}\\) is the modulus of \\[a_{j}\\], then [3] can be rewritten in the form:\n\\[\n\\mathbf{x} = a_{0}\\mathbf{e}_{0} + \\sum_{j = 1}^{\\lbrack\\frac{n - 1}{2}\\rbrack}{ {2^{1/2}r}_{j}{(\\mathbf{c}}_{j}\\cos\\theta_{j}{- \\mathbf{s}}_{j}\\sin\\theta_{j}) + a_{n/2}\\mathbf{e}_{n/2}}\n\\]\nThe orthonormal basis for \\[{R}^{n}\\] is \\[\\{\\mathbf{e}_{0},\\mathbf{c}_{1},\\mathbf{s}_{1},\\ldots,\\mathbf{c}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{s}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{e}_{\\frac{n}{2}(excluded\\ if\\ n\\ is\\ odd)}\\}\n\\], where:\n\\[\\mathbf{e}_{0}\\] is a vector composed of n elements equal to \\[n^{- 1/2}\\], which implies that \\[\\mathbf{a}_{0}\\mathbf{e}_{0} = {(n^{-1}\\sum_{t = 1}^{n}x_{t},\\ldots,n^{- 1}\\sum_{t=1}^{n}x_{t})}^{'}\\];\n$$\n{j}=()^{- 1/2}{({j},_{j},,)}^{’}, for 1 j \n$$ ;\n$$\n_{j} = {()}{-1/2}{(,,,)}{’}, for 1 j \n$$;\n$$\n_{n/2} = {(- (n{-}),n{- },,{-(n)}^{- }),n{-})}{’}\n$$.\nEquation [5] can be seen as an OLS regression of \\[x_{t}\\] on a constant and the trigonometric terms. As the vector of explanatory variables includes \\[n\\] elements, the number of explanatory variables in [5] is equal to the number of observations. HAMILTON, J.D. (1994) shows that the explanatory variables are linearly independent, which implies that an OLS regression yields a perfect fit (i.e. without an error term). The coefficients have the form of a simple OLS projection of the data on the orthonormal basis:\n$$\n{}{0}={t=1}^{n}x_{t} $$ [7] \n\\[\n  {\\widehat{a}}_{n/2}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}{(-1)}^{t}x_{t}(   \\text{only when n is even})\n  \\] [8] \n$$\n{}{0}={t=1}^{n}x_{t} $$ [9] \n\\[\n  {\\widehat{\\alpha}}_{j} = 2^{1/2}r_{j}\\cos{\\theta_{j}} = {(\\frac{n}{2})}^{- 1/2}\\sum_{t = 1}^{n}x_{t}\\cos{(t\\frac{2\\pi j}{n})}, j   = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack\n  \\] [10] \n$$\n{}{j} = 2^{1/2}r{j} = {()}^{-1/2}{t = 1}^{n}x{t}, j = 1,, $$ [11] \nWith [5] the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\) can be decomposed into \\[2 \\times \\lbrack\\frac{n - 1}{2}\\rbrack\\] components corresponding to \\[\\mathbf{c}_{j}\\] and \\[\\mathbf{s}_{j}\\], which are grouped to produce the “frequency \\[ω_{j}\\]” component for \\[1 \\geq j \\geq \\lbrack\\frac{n - 1}{2}\\rbrack\\]. As it is shown in the table below, the value of the periodogram at the frequency \\(\\omega_{j}\\) is the contribution of the$ j^{}$harmonic to the total sum of squares \\(\\sum_{t = 1}^{n}| x_{t} |^{2}\\).\nDecomposition of sum of squares into components corresponding to the harmonics\n{: .table .table-style} |Frequency |Degrees of freedom |Sum of squares decomposition| |———————————————– |———————— |————————————————————-| |\\(\\omega_{0}\\)(mean) |1 |\\[{a_{0}^{2}}_{\\ }=n^{- 1}(\\sum_{t=1}^{n}x_{t})^{2} = I( 0)\\]| |\\[\\omega_{1}\\] |2 |\\[{2r_{1}^{2}}_{\\ } = 2{\\|a_{1}\\|}^{2} = 2I(\\omega_{1})\\]| |\\[\\vdots\\] |\\[\\vdots\\] |\\[\\vdots\\]| |\\[\\omega_{k}\\] |2 |\\[{2r_{k}^{2}}_{\\ } = 2{\\|a_{k}\\|}^{2} = 2I(\\omega_{k})\\]| |\\[\\vdots\\] |\\[\\vdots\\] |\\[\\vdots\\]| |\\(\\omega_{n/2} = \\pi\\) (excluded if \\(n\\) is odd) |1 |\\[a_{n/2}^{2} = I(\\pi)\\]| |Total |\\[\\mathbf{n}\\] |\\[\\sum_{\\mathbf{t = 1}}^{\\mathbf{n}}\\mathbf{x}_{\\mathbf{t}}^{\\mathbf{2}}\\]|\nSource: DE ANTONIO, D., and PALATE, J. (2015).\nObviously, if series were random then each component $I(_{j})$would have the same expectation. On the contrary, when the series contains a systematic sine component having a frequency \\(j\\) and amplitude \\(A\\) then the sum of squares \\(I(\\omega_{j})\\) increases with \\(A\\). In practice, it is unlikely that the frequency \\(j\\) of an unknown systematic sine component would exacly match any of the frequencies, for which peridogram have been calcuated. Therefore, the periodogram would show an increase in intensities in the immediate vicinity of \\(j\\).8\nNote that in JDemetra+ the periodogram object corresponds exactly to the contribution to the sum of squares of the standardised data, since the series are divided by their standard deviation for computational reasons.\nUsing the decomposition presented in table above the periodogram can be expressed as:\n\\[\nI(\\omega_{j})\\mathbf{=}\\begin{matrix}                                                                                r_{j}^{2} = \\frac{1}{2}{(\\alpha}_{j}^{2} + \\beta_{j}^{2}) = \\ {\\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\cos{( {t\\frac{2\\pi j}{n}}_{\\ })\\ }})}^{2} + \\frac{1}{n}(\\sum_{t = 1}^{n}{x_{t}\\sin( t\\frac{2\\pi j}{n})_{\\ }})^{2} \\\\   \n\\end{matrix}\n\\] [12] \nwhere \\(j = 0,\\ldots,lbrack \\frac{n}{2} rbrack\\).\nSince \\(\\mathbf{x} - \\overline{\\mathbf{x}}\\) are generated by an orthonormal basis, and \\(\\overline{\\mathbf{x}}\\mathbf{=}a_{0}\\mathbf{e}_{0}\\) [5] can be rearranged to show that the sum of squares is equal to the sum of the squared coefficients:\n\\[\n\\mathbf{x} - a_{0}\\mathbf{e}_{0} =\\sum_{j=1}^{\\lbrack(n - 1)/2\\rbrack}(\\alpha_{j}\\mathbf{c}_{j}+\\beta_{j}\\mathbf{s}_{j}) + a_{n/2}\\mathbf{e}_{n/2}\n\\]. [13] \nThus the sample variance of \\[x_{t}\\] can be expressed as:\n$$\nn^{- 1}{t=1}{n}{(x_{t}-)}{2}=n{-1}({k=1}^{(n - 1)/2}2{r{j}}{2} +{a{n/2}}^{2})\n$$, [14] \nwhere \\(a_{n/2}^{2}\\) is excluded if \\(n\\) is odd.\nThe term \\[2{r_{j}}^{2}\\] in [14] is then the contribution of the \\(j^{\\text{th}}\\) harmonic to the variance and [14] shows then how the total variance is partitioned.\nThe periodogram ordinate \\(I(\\omega_{j})\\) and the autocovariance coefficient \\(\\gamma(k)\\) are both quadratic forms of \\[x_{t}\\]. It can be shown that the periodogram and autocovarinace function are related and the periodogram can be written in terms of the sample autocovariance function for any non-zero Fourier frequency \\[ω_{j}\\] :9\n$$\nI({j}) = {| k | < n}^{ }{( k)}{ }e^{- ik{j}} = {( 0)}{ } + 2{k = 1}^{n - 1}{( k)}_{ } $$\nand for the zero frequency \\(\\ I( 0) = n| \\overline{x} |^{2}\\).\nOnce comparing [15] with an expression for the spectral density of a stationary process:\n\\[\nf(\\omega_{\\ }) = \\frac{1}{2\\pi}\\sum_{k < - \\infty}^{\\infty}{\\gamma( k)}_{\\ }e^{- ik\\omega_{\\ }} = \\frac{1}{2\\pi}lbrack {\\gamma( 0)}_{\\ } + 2(\\sum_{k = 1}^{\\infty}{\\gamma( k)\\cos{(k\\omega_{\\ })}}) rbrack\n\\]\nIt can be noticed that the periodogram is a sample analog of the population spectrum. In fact, it can be shown that the periodogram is asymptotically unbiased but inconsistent estimator of the population spectrum \\(f(\\omega)\\).[^75] Therefore, the periodogram is a wildly fluctuating, with high variance, estimate of the spectrum. However, the consistent estimator can be achieved by applying the different linear smoothing filters to the periodogram, called lag-window estimators. The lag-window estimators implemented in JDemetra+ includes square, Welch, Tukey, Barlett, Hanning and Parzen. They are described in DE ANTONIO, D., and PALATE, J. (2015). Alternatively, the model-based consistent estimation procedure, resulting in autoregressive spectrum estimator, can be applied.\ncomment2: end part theory>spectral analysis>periodogram\n\n\n16.3.2 Method 2: Autoregressive spectrum estimation\nBROCKWELL, P.J., and DAVIS, R.A. (2006) point out that for any real-valued stationary process \\((x_{t})\\) with continuous spectral density \\(f(\\omega)\\) it is possible to find both \\(AR(p)\\) and \\(MA(q)\\) processes which spectral densities are arbitrarily close to \\(f(\\omega)\\). For this reason, in some sense, \\((x_{t})\\) can be approximated by either \\(AR(p)\\) or \\(MA(q)\\) process. This fact is a basis of one of the methods of achieving a consistent estimator of the spectrum, which is called an autoregressive spectrum estimation. It is based on the approximation of the stochastic process \\((x_{t})\\) by an autoregressive process of sufficiently high order \\(p\\):\n\\[\n  x_{t} = \\mu + (\\phi_{1}B + \\ldots + \\phi_{p}B^{p})x_{t} + \\varepsilon_{t}\n  \\]\nwhere \\[\\varepsilon_{t}\\] is a white-noise variable with mean zero and a constant variance.\nThe autoregressive spectrum estimator for the series \\(x_{t}\\) is defined as: 10\n$$\n() = 10 $$\nwhere:\n\\(\\omega\\)– frequency, \\(0 \\leq \\omega \\leq \\pi\\);\n\\(\\sigma_{x}^{2}\\) – the innovation variance of the sample residuals;\n\\[{\\widehat{\\phi}}_{k}\\] – \\(\\text{AR}(k)\\) coefficient estimates of the linear regression of \\(x_{t} - \\overline{x}\\) on \\(x_{t - k} - \\overline{x}\\), \\(1 \\leq k \\leq p\\).\nThe autoregressive spectrum estimator is used in the visual spectral analysis tool for detecting significant peaks in the spectrum. The criterion of visual significance, implemented in JDemetra+, is based on the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\) of the \\(\\widehat{s}(\\omega)\\) values, where \\({\\widehat{s}}^{\\max} = \\max_{k}\\widehat{s}(\\omega_{k})\\); \\({\\widehat{s}}^{\\min} = \\min_{k}\\widehat{s}(\\omega_{k});\\) and $(_{k})$is \\(k^{\\text{th}}\\) value of autoregressive spectrum estimator.\nThe particular value is considered to be visually significant if, at a trading day or at a seasonal frequency \\(\\omega_{k}\\) (other than the seasonal frequency \\(\\omega_{60} = \\pi\\)), $(_{k})$is above the median of the plotted values of \\(\\widehat{s}(\\omega_{k})\\) and is larger than both neighbouring values \\(\\widehat{s}(\\omega_{k - 1})\\) and \\(\\widehat{s}(\\omega_{k + 1})\\) by at least \\(\\frac{6}{52}\\) times the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\).\nFollowing the suggestion of SOUKUP, R.J., and FINDLEY, D.F. (1999), JDemetra+ uses an autoregressive model spectral estimator of model order 30. This order yields high resolution of strong components, meaning peaks that are sharply defined in the plot of \\(\\widehat{s}(\\omega)\\) with 61 frequencies. The minimum number of observations needed to compute the spectrum is set to \\(n=80\\) for monthly data and to \\(n =\\) 60 for quarterly series while the maximum number of observations considered for the estimation is 121. Consequently, with these settings it is possible to identify up to 30 peaks in the plot of 61 frequencies. By choosing \\(\\omega_{k} = \\frac{\\text{πk}}{60}\\) for $k = $0,1,…,60 the density estimates are calculated at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year).\nThe model order can also be selected based on the AIC criterion (in practice it is much lower than 30). A lower order produces the smoother spectrum, but the contrast between the spectral amplitudes at the trading day frequencies and neighbouring frequencies is weaker, and therefore not as suitable for automatic detection.\nSOUKUP, R.J., and FINDLEY, D.F. (1999) also explain that the periodogram can be used in the visual significance test as it has as good as those of the AR(30) spectrum abilities to detect trading day effect, but also has a greater false alarm rate11.\ncomment2: end part theory>spectral analysis>auto-regressive spectrum\n\n\n16.3.3 Method 3: Tukey spectrum"
  },
  {
    "objectID": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "href": "M-spectral-analysis.html#identification-of-spectral-peaks",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.4 Identification of spectral peaks",
    "text": "16.4 Identification of spectral peaks\ncomment3: start part theory>spectral analysis>identification of spectral peaks\nIdentification of seasonal peaks in a Tukey periodogram and in an autoregressive spectrum\nIn order to decide whether a series has a seasonal component that is predictable (stable) enough, these tests use visual criteria and formal tests for the periodogram. The periodogram is calculated using complete years, so that the set of Fourier frequencies contains exactly all seasonal frequencies12.\nThe tests rely on two basic principles:\n\nThe peaks associated with seasonal frequencies should be larger than the median spectrum for all frequencies and;\nThe peaks should exceed the spectrum of the two adjacent values by more than a critical value.\n\n\nJDemetra+ performs this test on the original series. If these two requirements are met, the test results are displayed in green. The statistical significance of each of the seasonal peaks (i.e. frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3}\\) and $$ corresponding to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal and trading days frequencies depends on the frequency of time series. They are shown in the table below. The symbol \\(d\\) denotes a default frequency and is described below the table.\n\nThe seasonal and trading day frequencies by time series frequency\n{: .table .table-style} |Number of months per full period | Seasonal frequency | Trading day frequency (radians)| |————————————–| ————————————————————————————-| ————————————| |12 | \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) | \\(d\\), 2.714| |6 | \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) | \\[d\\] |4 | \\(\\frac{\\pi}{2}\\), \\(\\pi\\) | \\(d\\), 1.292, 1.850, 2.128| |3 | \\[\\pi\\] | \\[d\\]| |2 | \\[\\pi\\] | \\[d\\]|\nThe calendar (trading day or working day) effects, related to the variation in the number of different days of the week per period, can induce periodic patterns in the data that can be similar to those resulting from pure seasonal effects. From the theoretical point of view, trading day variability is mainly due to the fact that the average number of days in the months or quarters is not equal to a multiple of 7 (the average number of days of a month in the year of 365.25 days is equal to \\(\\frac{365.25}{12} =\\) 30.4375 days). This effect occurs \\(\\frac{365.25}{12} \\times \\frac{1}{7} =\\) 4.3482 times per month: one time for each one of the four complete weeks of each month, and a residual of 0.3482 cycles per month, i.e. \\(0.3482 \\times 2\\pi = 2.1878\\) radians. This turns out to be a fundamental frequency for the effects associated with monthly data. In JDemetra+ the fundamental frequency corresponding to 0.3482 cycles per month is used in place of the closest frequency \\(\\frac{\\text{πk}}{60}\\). Thus, the quantity \\(\\frac{\\pi \\times 42}{60}\\) is replaced by \\(\\omega_{42} = 0.3482 \\times 2\\pi = 2.1878\\). The frequencies neighbouring \\(\\omega_{42}\\), i.e. \\(\\omega_{41}\\) and \\(\\omega_{43}\\) are set to, respectively, \\(2.1865 - \\frac{1}{60}\\) and \\(2.1865 + \\frac{1}{60}\\).\nThe default frequencies (\\(d)\\ \\)for calendar effect are: 2.188 (monthly series) and 0.280 (quarterly series). They are computed as:\n\\[\n\\omega_{\\text{ce}} = \\frac{2\\pi}{7}\\left( n - 7 \\times \\left\\lbrack \\frac{n}{7} \\right\\rbrack \\right)\n\\], [1] \nwhere:\n\\(n = \\frac{365.25}{s}\\), \\(s = 4\\) for quarterly series and \\(s = 12\\) for monthly series.\nOther frequencies that correspond to trading day frequencies are: 2.714 (monthly series) and 1.292, 1.850, 2.128 (quarterly series).\nIn particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4 cycles per year \\(\\text{ω}_{40} = \\frac{2}{3}\\pi = 2.0944\\).\n\n\n\nText\n\n\nPeriodogram with seasonal (grey) and calendar (red) frequencies highlighted\nThis implies that it may be hard to disentangle both effects using the frequency domain techniques.\ncomment3: end part theory>spectral analysis>identification of spectral peaks"
  },
  {
    "objectID": "M-spectral-analysis.html#spectral-graphs",
    "href": "M-spectral-analysis.html#spectral-graphs",
    "title": "16  Spectral Analysis Principles and Tools",
    "section": "16.5 Spectral graphs",
    "text": "16.5 Spectral graphs\ncomment3: start part case studies > spectral graphs\nThis scenario is designed for advanced users interested in an in-depth analysis of time series in the frequency domain using three spectral graphs. Those graphs can also be used as a complementary analysis for a better understanding of the results obtained with some of the tests described above.\nEconomic time series are usually presented in a time domain (X-axis). However, for analytical purposes it is convenient to convert the series to a frequency domain due to the fact that any stationary time series can be expressed as a combination of cosine (or sine) functions. These functions are characterized with different periods (amount of time to complete a full cycle) and amplitudes (maximum/minimum value during the cycle).\nThe tool used for the analysis of a time series in a frequency domain is called a spectrum. The peaks in the spectrum indicate the presence of cyclical movements with periodicity between two months and one year. A seasonal series should have peaks at the seasonal frequencies. Calendar adjusted data are not expected to have peak at with a calendar frequency.\nThe periodicity of the phenomenon at frequency f is \\(\\frac{2\\pi}{f}\\). It means that for a monthly time series the seasonal frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3},\\ \\frac{5\\pi}{6}\\ \\) and \\(\\pi\\) correspond to 1, 2, 3, 4, 5 and 6 cycles per year. For example, the frequency \\(\\frac{\\pi}{3}\\) corresponds to a periodicity of 6 months (2 cycles per year are completed). For the quarterly series there are two seasonal frequencies: \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). A peak at the zero frequency always corresponds to the trend component of the series. Seasonal frequencies are marked as grey vertical lines, while violet vertical lines represent the trading-days frequencies. The trading day frequency is 0.348 and derives from the fact that a daily component which repeats every seven days goes through 4.348 cycles in a month of average length 30.4375 days. It is therefore seen to advance 0.348 cycles per month when the data are obtained at twelve equally spaced times in 365.25 days (the average length of a year).\nThe interpretation of the spectral graph is rather straightforward. When the values of a spectral graph for low frequencies (i.e. one year and more) are large in relation to its other values it means that the long-term movements dominate in the series. When the values of a spectral graph for high frequencies (i.e. below one year) are large in relation to its other values it means that the series are rather trendless and contains a lot of noise. When the values of a spectral graph are distributed randomly around a constant without any visible peaks, then it is highly probable that the series is a random process. The presence of seasonality in a time series is manifested in a spectral graph by the peaks on the seasonal frequencies.\nLACK : all links to graph and GUI: cfr old doc\ncomment3: end part case studies > spectral graphs"
  },
  {
    "objectID": "M-reg-arima-modelling.html",
    "href": "M-reg-arima-modelling.html",
    "title": "17  Reg-Arima models",
    "section": "",
    "text": "lot of information might be recycled from the old online documentation cf file 18-Meth-Reg-Arima-Modelling.Rmd where info from the old pages is gathered formulas and tables compatibility with quarto have to be checked before pasting in the book\nIn the chapter on SA, in the pre-adjustment section, we tackle: purpose, principles and results of reg-arima models (tramo or reg-arima)\nObjectives of the chapter: * all the technical non SA specific details * differences (none left ?) between Tramo and Reg-Arima (X13)"
  },
  {
    "objectID": "M-reg-arima-modelling.html#regarima-model",
    "href": "M-reg-arima-modelling.html#regarima-model",
    "title": "17  Reg-Arima models",
    "section": "17.2 RegARIMA model",
    "text": "17.2 RegARIMA model\nThe primary aim of seasonal adjustment is to remove the unobservable seasonal component from the observed series. The decomposition routines implemented in the seasonal adjustment methods make specific assumptions concerning the input series. One of the crucial assumptions is that the input series is stochastic, i.e. it is clean of deterministic effects. Another important limitation derives from the symmetric linear filter used in TRAMO-SEATS and X-13ARIMA-SEATS. A symmetric linear filter cannot be applied to the first and last observations with the same set of weights as for the central observations[^1]. Therefore, for the most recent observations these filters provide estimates that are subject to revisions.\nTo overcome these constrains both seasonal adjustment methods discussed here include a modelling step that aims to analyse the time series development and provide a better input for decomposition purposes. The tool that is frequently used for this purpose is the ARIMA model, as discussed by BOX, G.E.P., and JENKINS, G.M. (1970). However, time series are often affected by the outliers, other deterministic effects and missing observations. The presence of these effects is not in line with the ARIMA model assumptions. The presence of outliers and other deterministic effects impede the identification of an optimal ARIMA model due to the important bias in the estimation of parameters of sample autocorrelation functions (both global and partial)[^3]. Therefore, the original series need to be corrected for any deterministic effects and missing observations. This process is called linearisation and results in the stochastic series that can be modelled by ARIMA.\nFor this purpose both TRAMO and RegARIMA use regression models with ARIMA errors. With these models TRAMO and RegARIMA also produce forecasts."
  },
  {
    "objectID": "M-moving-average-decomposition.html",
    "href": "M-moving-average-decomposition.html",
    "title": "18  Moving average based decomposition",
    "section": "",
    "text": "a lot of information might be recycled from the old online documentation cf file 19-Meth-Local-decomposition.Rmd where info from the old pages is gathered formulas and tables compatibility with quarto have to be checked before pasting in the book\nlinks to rjdfilters"
  },
  {
    "objectID": "M-local-regression-decomposition.html",
    "href": "M-local-regression-decomposition.html",
    "title": "19  Local regression decomposition",
    "section": "",
    "text": "goal of the chapter : details on STL which are not in the SA chapter"
  },
  {
    "objectID": "M-amb-decomposition.html",
    "href": "M-amb-decomposition.html",
    "title": "20  Arima Model Based AMB decomposition",
    "section": "",
    "text": "goal of the chapter : details on SEATS which are not in the SA chapter\na lot of information might be recycled from the old online documentation cf file 20-Meth-AMB-decomposition.Rmd where info from the old pages is gathered formulas and tables compatibility with quarto have to be checked before pasting in the book"
  },
  {
    "objectID": "Buffer.html",
    "href": "Buffer.html",
    "title": "22  Theoretical spectral density of an ARIMA model",
    "section": "",
    "text": "In the bottom part the panel the ARIMA model used by TRAMO is presented using symbolic notation \\((P,D,Q)(PB,DB,QB)\\). Estimated parameters’ coefficients (regular and seasonal AR and MA) are shown in closed form (i.e. using the backshift operator1 \\(B\\)). For each regular AR root (i.e. the solution of the characteristic equation) the argument and modulus are given.\n\n\n\nText\n\n\nThe details of ARIMA model used for modelling\nFor each regular AR root the argument and modulus are also reported (if present, i.e. if \\(\\)) to inform to which time series component the regular roots would be assigned.\n\n\n\n\n\nA backshift operator \\(B \\)is defined as: (\\(B^{k}x_{t} = x_{t - k})\\). It is used to denote lagged series.↩︎"
  }
]